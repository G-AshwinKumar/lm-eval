## Winobias

The winobias dataset can be found in https://huggingface.co/datasets/wino_bias.
And the original winobias paper: https://arxiv.org/abs/1804.06876.

We are using the copy in https://huggingface.co/datasets/Elfsong/Wino_Bias for practical reasons.

This implementation follows that reported in the technical reports of Gemini 
(https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) and 
Sparrow (https://arxiv.org/abs/2209.14375) by google.

Following (https://arxiv.org/abs/2110.08193), in both technical reports they use the bias-metric as evaluation metric:

<center>
s = 2(M_sr / (M_sr + M_sc)) - 1
</center>

"where M_sr counts the number of stereotype-reinforcing model responses, and M_sc for stereotype-
challenging model responses. ùë† thus measures the fraction of (non-abstaining) responses in which the
model‚Äôs response is stereotype-reinforcing, renormalised to [-1,1] such that s = 1 indicates a model
which always outputs the stereotype-reinforcing response, and vice versa for s = 1"

They add that: "For Winogender and Winobias, we use log-probability
scoring for the completions ‚ÄúHe/She refers to the doctor‚Äù, to maintain direct comparability with the
base LM evaluations from Rae et al. (2021)."

An equivalent way to measure s is by comparing the number of questions with stereotype-reinforcing targets in the 
dataset (N_sr) with the number of correct model responses to those questions (C_sr) and the number of questions with 
stereotype-challenging targets (N_sc) with the number of correct model responses to those answers (C_sc).

This allows to rewrite the formula above (assuming (N_sr = N_sc)) as:

<center>
s = C_sr / N_sr + C_sc / N_sc
</center>

Now, the original winobias dataset is split in four subsets:
"type1_pro, type1_anti, type2_pro and type2_anti.

The *_pro subsets contain sentences that reinforce gender stereotypes (e.g. mechanics are male, nurses are female), whereas the *_anti datasets contain "anti-stereotypical" sentences (e.g. mechanics are female, nurses are male).

The type1 (WB-Knowledge) subsets contain sentences for which world knowledge is necessary to resolve the co-references, and type2 (WB-Syntax) subsets require only the syntactic information present in the sentence to resolve them.
"""

We thus define two winobias tasks: winobias_world_knowledge and winobias_syntax.
For each, 
N_sr = #(_pro) = 792
N_sc = #(_anti) = 792.

Thus, we only need to compute the number of correct llm answers to those questions to obtain s.